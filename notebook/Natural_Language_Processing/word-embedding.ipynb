{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fea2d2",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Install the necessary dependencies\n",
    "\n",
    "import os\n",
    "import sys\n",
    "!{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython gensim torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74660f15",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "---\n",
    "license:\n",
    "    code: MIT\n",
    "    content: CC-BY-4.0\n",
    "github: https://github.com/ocademy-ai/machine-learning\n",
    "venue: By Ocademy\n",
    "open_access: true\n",
    "bibliography:\n",
    "  - https://raw.githubusercontent.com/ocademy-ai/machine-learning/main/open-machine-learning-jupyter-book/references.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79b007",
   "metadata": {},
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef3de21",
   "metadata": {},
   "source": [
    "Word Embeddings are numeric representations of words in a lower-dimensional space, capturing semantic and syntactic information. Mainly including discrete representation methods and distribution representation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf1c07c",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e6d642",
   "metadata": {},
   "source": [
    "Word2Vec is a neural approach for generating word embeddings. It belongs to the family of neural word embedding techniques and specifically falls under the category of distributed representation models. It is a popular technique in natural language processing (NLP) that is used to represent words as continuous vector spaces. Developed by a team at Google, Word2Vec aims to capture the semantic relationships between words by mapping them to high-dimensional vectors. The underlying idea is that words with similar meanings should have similar vector representations. In Word2Vec every word is assigned a vector. We start with either a random vector or one-hot vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf16b0de",
   "metadata": {},
   "source": [
    "There are two neural embedding methods for Word2Vec, Continuous Bag of Words (CBOW) and Skip-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba4b22",
   "metadata": {},
   "source": [
    "### Continuous Bag of Words(CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d21ea",
   "metadata": {},
   "source": [
    "Continuous Bag of Words (CBOW) is a type of neural network architecture used in the Word2Vec model. The primary objective of CBOW is to predict a target word based on its context, which consists of the surrounding words in a given window. Given a sequence of words in a context window, the model is trained to predict the target word at the center of the window.\n",
    "\n",
    "CBOW is a feedforward neural network with a single hidden layer. The input layer represents the context words, and the output layer represents the target word. The hidden layer contains the learned continuous vector representations (word embeddings) of the input words.\n",
    "\n",
    "The architecture is useful for learning distributed representations of words in a continuous vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e81f1f",
   "metadata": {},
   "source": [
    "<center><img src= https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/cbow.png width: 20% class=\"bg-white mb-1\"><br>Image: Continuous Bag of Words</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a2f35",
   "metadata": {},
   "source": [
    "The hidden layer contains the continuous vector representations (word embeddings) of the input words.\n",
    "\n",
    "The weights between the input layer and the hidden layer are learned during training.\n",
    "The dimensionality of the hidden layer represents the size of the word embeddings (the continuous vector space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c51a75d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0\n",
      "Epoch 2, Loss: 0\n",
      "Epoch 3, Loss: 0\n",
      "Epoch 4, Loss: 0\n",
      "Epoch 5, Loss: 0\n",
      "Epoch 6, Loss: 0\n",
      "Epoch 7, Loss: 0\n",
      "Epoch 8, Loss: 0\n",
      "Epoch 9, Loss: 0\n",
      "Epoch 10, Loss: 0\n",
      "Epoch 11, Loss: 0\n",
      "Epoch 12, Loss: 0\n",
      "Epoch 13, Loss: 0\n",
      "Epoch 14, Loss: 0\n",
      "Epoch 15, Loss: 0\n",
      "Epoch 16, Loss: 0\n",
      "Epoch 17, Loss: 0\n",
      "Epoch 18, Loss: 0\n",
      "Epoch 19, Loss: 0\n",
      "Epoch 20, Loss: 0\n",
      "Epoch 21, Loss: 0\n",
      "Epoch 22, Loss: 0\n",
      "Epoch 23, Loss: 0\n",
      "Epoch 24, Loss: 0\n",
      "Epoch 25, Loss: 0\n",
      "Epoch 26, Loss: 0\n",
      "Epoch 27, Loss: 0\n",
      "Epoch 28, Loss: 0\n",
      "Epoch 29, Loss: 0\n",
      "Epoch 30, Loss: 0\n",
      "Epoch 31, Loss: 0\n",
      "Epoch 32, Loss: 0\n",
      "Epoch 33, Loss: 0\n",
      "Epoch 34, Loss: 0\n",
      "Epoch 35, Loss: 0\n",
      "Epoch 36, Loss: 0\n",
      "Epoch 37, Loss: 0\n",
      "Epoch 38, Loss: 0\n",
      "Epoch 39, Loss: 0\n",
      "Epoch 40, Loss: 0\n",
      "Epoch 41, Loss: 0\n",
      "Epoch 42, Loss: 0\n",
      "Epoch 43, Loss: 0\n",
      "Epoch 44, Loss: 0\n",
      "Epoch 45, Loss: 0\n",
      "Epoch 46, Loss: 0\n",
      "Epoch 47, Loss: 0\n",
      "Epoch 48, Loss: 0\n",
      "Epoch 49, Loss: 0\n",
      "Epoch 50, Loss: 0\n",
      "Epoch 51, Loss: 0\n",
      "Epoch 52, Loss: 0\n",
      "Epoch 53, Loss: 0\n",
      "Epoch 54, Loss: 0\n",
      "Epoch 55, Loss: 0\n",
      "Epoch 56, Loss: 0\n",
      "Epoch 57, Loss: 0\n",
      "Epoch 58, Loss: 0\n",
      "Epoch 59, Loss: 0\n",
      "Epoch 60, Loss: 0\n",
      "Epoch 61, Loss: 0\n",
      "Epoch 62, Loss: 0\n",
      "Epoch 63, Loss: 0\n",
      "Epoch 64, Loss: 0\n",
      "Epoch 65, Loss: 0\n",
      "Epoch 66, Loss: 0\n",
      "Epoch 67, Loss: 0\n",
      "Epoch 68, Loss: 0\n",
      "Epoch 69, Loss: 0\n",
      "Epoch 70, Loss: 0\n",
      "Epoch 71, Loss: 0\n",
      "Epoch 72, Loss: 0\n",
      "Epoch 73, Loss: 0\n",
      "Epoch 74, Loss: 0\n",
      "Epoch 75, Loss: 0\n",
      "Epoch 76, Loss: 0\n",
      "Epoch 77, Loss: 0\n",
      "Epoch 78, Loss: 0\n",
      "Epoch 79, Loss: 0\n",
      "Epoch 80, Loss: 0\n",
      "Epoch 81, Loss: 0\n",
      "Epoch 82, Loss: 0\n",
      "Epoch 83, Loss: 0\n",
      "Epoch 84, Loss: 0\n",
      "Epoch 85, Loss: 0\n",
      "Epoch 86, Loss: 0\n",
      "Epoch 87, Loss: 0\n",
      "Epoch 88, Loss: 0\n",
      "Epoch 89, Loss: 0\n",
      "Epoch 90, Loss: 0\n",
      "Epoch 91, Loss: 0\n",
      "Epoch 92, Loss: 0\n",
      "Epoch 93, Loss: 0\n",
      "Epoch 94, Loss: 0\n",
      "Epoch 95, Loss: 0\n",
      "Epoch 96, Loss: 0\n",
      "Epoch 97, Loss: 0\n",
      "Epoch 98, Loss: 0\n",
      "Epoch 99, Loss: 0\n",
      "Epoch 100, Loss: 0\n",
      "Embedding for 'embeddings': [[-1.0344244   0.07897425  0.03608504 -0.556515    0.14206451 -1.833232\n",
      "   0.7083351   0.66258     0.57552516  1.7009653 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define CBOW model\n",
    "class CBOWModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embed_size):\n",
    "\t\tsuper(CBOWModel, self).__init__()\n",
    "\t\tself.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "\t\tself.linear = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "\tdef forward(self, context):\n",
    "\t\tcontext_embeds = self.embeddings(context).sum(dim=1)\n",
    "\t\toutput = self.linear(context_embeds)\n",
    "\t\treturn output\n",
    "\n",
    "# Sample data\n",
    "context_size = 2\n",
    "raw_text = \"word embeddings are awesome\"\n",
    "tokens = raw_text.split()\n",
    "vocab = set(tokens)\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(tokens) - 2):\n",
    "\tcontext = [word_to_index[word] for word in tokens[i - 2:i] + tokens[i + 1:i + 3]]\n",
    "\ttarget = word_to_index[tokens[i]]\n",
    "\tdata.append((torch.tensor(context), torch.tensor(target)))\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Initialize CBOW model\n",
    "cbow_model = CBOWModel(vocab_size, embed_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "\ttotal_loss = 0\n",
    "\tfor context, target in data:\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutput = cbow_model(context)\n",
    "\t\tloss = criterion(output.unsqueeze(0), target.unsqueeze(0))\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\ttotal_loss += loss.item()\n",
    "\tprint(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n",
    "\n",
    "# Example usage: Get embedding for a specific word\n",
    "word_to_lookup = \"embeddings\"\n",
    "word_index = word_to_index[word_to_lookup]\n",
    "embedding = cbow_model.embeddings(torch.tensor([word_index]))\n",
    "print(f\"Embedding for '{word_to_lookup}': {embedding.detach().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f057334a",
   "metadata": {},
   "source": [
    "### Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b64f9",
   "metadata": {},
   "source": [
    "The Skip-Gram model learns distributed representations of words in a continuous vector space. The main objective of Skip-Gram is to predict context words (words surrounding a target word) given a target word. This is the opposite of the Continuous Bag of Words (CBOW) model, where the objective is to predict the target word based on its context. It is shown that this method produces more meaningful embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f31a42",
   "metadata": {},
   "source": [
    "<center><img src= https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/skipgram.png width: 60% class=\"bg-white mb-1\"><br>Image: NER in NLP</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b2fa3",
   "metadata": {},
   "source": [
    "After applying the above neural embedding methods we get trained vectors of each word after many iterations through the corpus. These trained vectors preserve syntactical or semantic information and are converted to lower dimensions. The vectors with similar meaning or semantic information are placed close to each other in space.\n",
    "\n",
    "Letâ€™s understand with a basic example. The python code contains, parameter that controls the dimensionality of the word vectors, and you can adjust other parameters such as based on your specific needs.vector_sizewindow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc4e0e",
   "metadata": {},
   "source": [
    ">Note: Word2Vec models can perform better with larger datasets. \n",
    ">If you have a large corpus, you might achieve more meaningful word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbe003fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in d:\\anaconda\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in d:\\anaconda\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\anaconda\\lib\\site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in d:\\anaconda\\lib\\site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\anaconda\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: pyfume in d:\\anaconda\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\anaconda\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Requirement already satisfied: fst-pso in d:\\anaconda\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: simpful in d:\\anaconda\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.11.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in d:\\anaconda\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhongmeiqi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'word': [-9.5800208e-03  8.9437785e-03  4.1664648e-03  9.2367809e-03\n",
      "  6.6457358e-03  2.9233587e-03  9.8055992e-03 -4.4231843e-03\n",
      " -6.8048164e-03  4.2256550e-03  3.7299085e-03 -5.6668529e-03\n",
      "  9.7035142e-03 -3.5551414e-03  9.5499391e-03  8.3657773e-04\n",
      " -6.3355025e-03 -1.9741615e-03 -7.3781307e-03 -2.9811086e-03\n",
      "  1.0425397e-03  9.4814906e-03  9.3598543e-03 -6.5986011e-03\n",
      "  3.4773252e-03  2.2767992e-03 -2.4910474e-03 -9.2290826e-03\n",
      "  1.0267317e-03 -8.1645092e-03  6.3240929e-03 -5.8001447e-03\n",
      "  5.5353874e-03  9.8330071e-03 -1.5987856e-04  4.5296676e-03\n",
      " -1.8086446e-03  7.3613892e-03  3.9419360e-03 -9.0095028e-03\n",
      " -2.3953868e-03  3.6261671e-03 -1.0080514e-04 -1.2024897e-03\n",
      " -1.0558038e-03 -1.6681013e-03  6.0541567e-04  4.1633579e-03\n",
      " -4.2531900e-03 -3.8336846e-03 -5.0755290e-05  2.6549282e-04\n",
      " -1.7014991e-04 -4.7843382e-03  4.3120929e-03 -2.1710952e-03\n",
      "  2.1056964e-03  6.6702347e-04  5.9686624e-03 -6.8418151e-03\n",
      " -6.8183104e-03 -4.4762432e-03  9.4359247e-03 -1.5930856e-03\n",
      " -9.4291316e-03 -5.4270827e-04 -4.4478951e-03  5.9980620e-03\n",
      " -9.5831212e-03  2.8602476e-03 -9.2544509e-03  1.2484600e-03\n",
      "  6.0004774e-03  7.4001122e-03 -7.6209377e-03 -6.0561695e-03\n",
      " -6.8399287e-03 -7.9184016e-03 -9.4984965e-03 -2.1255787e-03\n",
      " -8.3757477e-04 -7.2564054e-03  6.7876028e-03  1.1183097e-03\n",
      "  5.8291717e-03  1.4714618e-03  7.9081533e-04 -7.3718326e-03\n",
      " -2.1769912e-03  4.3199472e-03 -5.0856168e-03  1.1304744e-03\n",
      "  2.8835384e-03 -1.5386029e-03  9.9318363e-03  8.3507905e-03\n",
      "  2.4184163e-03  7.1170190e-03  5.8888551e-03 -5.5787875e-03]\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt') # Download the tokenizer models if not already downloaded\n",
    "\n",
    "sample = \"Word embeddings are dense vector representations of words.\"\n",
    "tokenized_corpus = word_tokenize(sample.lower()) # Lowercasing for consistency\n",
    "\n",
    "skipgram_model = Word2Vec(sentences=[tokenized_corpus],\n",
    "\t\t\t\t\t\tvector_size=100, # Dimensionality of the word vectors\n",
    "\t\t\t\t\t\twindow=5,\t\t # Maximum distance between the current and predicted word within a sentence\n",
    "\t\t\t\t\t\tsg=1,\t\t\t # Skip-Gram model (1 for Skip-Gram, 0 for CBOW)\n",
    "\t\t\t\t\t\tmin_count=1,\t # Ignores all words with a total frequency lower than this\n",
    "\t\t\t\t\t\tworkers=4)\t # Number of CPU cores to use for training the model\n",
    "\n",
    "# Training\n",
    "skipgram_model.train([tokenized_corpus], total_examples=1, epochs=10)\n",
    "skipgram_model.save(\"skipgram_model.model\")\n",
    "loaded_model = Word2Vec.load(\"skipgram_model.model\")\n",
    "vector_representation = loaded_model.wv['word']\n",
    "print(\"Vector representation of 'word':\", vector_representation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cda55f",
   "metadata": {},
   "source": [
    "In practice, the choice between CBOW and Skip-gram often depends on the specific characteristics of the data and the task at hand. CBOW might be preferred when training resources are limited, and capturing syntactic information is important. Skip-gram, on the other hand, might be chosen when semantic relationships and the representation of rare words are crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c887969",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a6505",
   "metadata": {},
   "source": [
    "GloVe is trained on global word co-occurrence statistics. It leverages the global context to create word embeddings that reflect the overall meaning of words based on their co-occurrence probabilities. this method, we take the corpus and iterate through it and get the co-occurrence of each word with other words in the corpus. We get a co-occurrence matrix through this. The words which occur next to each other get a value of 1, if they are one word apart then 1/2, if two words apart then 1/3 and so on.\n",
    "\n",
    "Let us take an example to understand how the matrix is created. We have a small corpus:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94e71e",
   "metadata": {},
   "source": [
    ">Corpus:\n",
    ">\n",
    ">It is a nice evening.\n",
    ">\n",
    ">Good Evening!\n",
    ">\n",
    ">Is it a nice evening?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394c071b",
   "metadata": {},
   "source": [
    "| |it|is|a|nice|evening|good|\n",
    "|----------|----------|----------|----------|----------|----------|----------|\n",
    "|it|0|\n",
    "|is|1+1|0|\n",
    "|a|1/2+1|1+1/2|0|\n",
    "|nice|1/3+1/2|1/2+1/3|1+1|0|\n",
    "|evening|1/4+1/3|1/3+1/4|1/2+1/2|1+1|0|\n",
    "|good|0|0|0|0|1|0|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd03226",
   "metadata": {},
   "source": [
    "The upper half of the matrix will be a reflection of the lower half. We can consider a window frame as well to calculate the co-occurrences by shifting the frame till the end of the corpus. This helps gather information about the context in which the word is used.\n",
    "\n",
    "Initially, the vectors for each word is assigned randomly. Then we take two pairs of vectors and see how close they are to each other in space. If they occur together more often or have a higher value in the co-occurrence matrix and are far apart in space then they are brought close to each other. If they are close to each other but are rarely or not frequently used together then they are moved further apart in space.\n",
    "\n",
    "After many iterations of the above process, weâ€™ll get a vector space representation that approximates the information from the co-occurrence matrix. The performance of GloVe is better than Word2Vec in terms of both semantic and syntactic capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3737fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.downloader import load\n",
    "\n",
    "glove_model = load('glove-wiki-gigaword-50')\n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
    "\n",
    "# Compute similarity for each pair of words\n",
    "for pair in word_pairs:\n",
    "\tsimilarity = glove_model.similarity(pair[0], pair[1])\n",
    "\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using GloVe: {similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c55b60",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    ">Similarity between 'learn' and 'learning' using GloVe: 0.802\n",
    ">\n",
    ">Similarity between 'india' and 'indian' using GloVe: 0.865\n",
    ">\n",
    ">Similarity between 'fame' and 'famous' using GloVe: 0.589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "991ba541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p style=\"text-align: center;\">\n",
       "<iframe src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/html/embedding-explorer/Embedding%20Explorer.html\" width=\"100%\" height=\"800px;\"\n",
       "style=\"border:none;\" scrolling=\"auto\"></iframe>\n",
       "A demo of convolution function. <a\n",
       "href=\"https://cs231n.github.io/convolutional-networks/\"> [source]</a>\n",
       "</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "display(HTML(\"\"\"\n",
    "<p style=\"text-align: center;\">\n",
    "<iframe src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/html/embedding-explorer/Embedding%20Explorer.html\" width=\"100%\" height=\"800px;\"\n",
    "style=\"border:none;\" scrolling=\"auto\"></iframe>\n",
    "A demo of convolution function. <a\n",
    "href=\"https://cs231n.github.io/convolutional-networks/\"> [source]</a>\n",
    "</p>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ab78a",
   "metadata": {},
   "source": [
    "<center><img src= https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/embedding-explorer.png width: 60% class=\"bg-white mb-1\"><br>Image: embedding visualization</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15dc1cd",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantage of Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef3ca2",
   "metadata": {},
   "source": [
    "### Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3250120",
   "metadata": {},
   "source": [
    "+ It is much faster to train than hand build models like WordNet (which uses graph embeddings).\n",
    "+ Almost all modern NLP applications start with an embedding layer.\n",
    "+ It Stores an approximation of meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43e912",
   "metadata": {},
   "source": [
    "### Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57b362a",
   "metadata": {},
   "source": [
    "+ It can be memory intensive.\n",
    "+ It is corpus dependent. Any underlying bias will have an effect on your model.\n",
    "+ It cannot distinguish between homophones. Eg: brake/break, cell/sell, weather/whether etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13556362",
   "metadata": {},
   "source": [
    "## Your turn! ðŸš€\n",
    "\n",
    "Assignment - [News topic classification tasks](assignments/news-topic-classification-tasks.ipynb)\n",
    "\n",
    "Assignment - [Project: Classify Medium Articles with Embeddings](assignments/embedding_project.ipynb)\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Thanks to [GeeksforGeeks](https://auth.geeksforgeeks.org/user/shristikotaiah/articles?utm_source=geeksforgeeks&utm_medium=article_author&utm_campaign=auth_user) for creating the open-source project [Word Embeddings in NLP](https://www.geeksforgeeks.org/word-embeddings-in-nlp).It inspire the majority of the content in this chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
