{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Classify Medium Articles with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s upgrade our **text classification model** as well by leveraging **sentence embeddings**. The scope of the project is to build a text classification model (a simple logistic regression) leveraging sentence embeddings, capable of distinguishing **whether a text is about data science or not**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\anaconda\\lib\\site-packages (2.13.1)\n",
      "Requirement already satisfied: sentence-transformers in d:\\anaconda\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\anaconda\\lib\\site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\anaconda\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in d:\\anaconda\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in d:\\anaconda\\lib\\site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\anaconda\\lib\\site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\anaconda\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in d:\\anaconda\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: Pillow in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.10.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.35.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence_transformers in d:\\anaconda\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from sentence_transformers) (1.10.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\lib\\site-packages (from sentence_transformers) (1.2.1)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from sentence_transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in d:\\anaconda\\lib\\site-packages (from sentence_transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from sentence_transformers) (1.23.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\anaconda\\lib\\site-packages (from sentence_transformers) (2.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in d:\\anaconda\\lib\\site-packages (from sentence_transformers) (4.35.2)\n",
      "Requirement already satisfied: Pillow in d:\\anaconda\\lib\\site-packages (from sentence_transformers) (9.4.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2022.11.0)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (2.8.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\anaconda\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets sentence-transformers\n",
    "%pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,ConfusionMatrixDisplay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset of [Medium articles from the Hugging Face Hub](https://huggingface.co/datasets/fabiochiu/medium-articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/fabiochiu/medium-articles/resolve/main/medium_articles.csv (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000202DAF2F250>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 22350bb7-c1b9-4561-baf3-ca551d9583c9)')' thrown while requesting HEAD https://huggingface.co/datasets/fabiochiu/medium-articles/resolve/main/medium_articles.csv\n"
     ]
    },
    {
     "ename": "LocalEntryNotFoundError",
     "evalue": "Connection error, and we cannot find the requested files in the disk cache. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df_articles \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m----> 2\u001b[0m   \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfabiochiu/medium-articles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedium_articles.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m df_articles\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32md:\\anaconda\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\lib\\site-packages\\huggingface_hub\\file_download.py:1291\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1285\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[0;32m   1286\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find the requested files in the disk cache and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1287\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m outgoing traffic has been disabled. To enable hf.co look-ups\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1288\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and downloads online, set \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1289\u001b[0m         )\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1291\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[0;32m   1292\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection error, and we cannot find the requested files in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the disk cache. Please try again or make sure your Internet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m connection is on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;66;03m# From now on, etag and commit_hash are not None.\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m etag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metag must have been retrieved from server\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m: Connection error, and we cannot find the requested files in the disk cache. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "df_articles = pd.read_csv(\n",
    "  hf_hub_download(\"fabiochiu/medium-articles\", repo_type=\"dataset\", filename=\"medium_articles.csv\")\n",
    ")\n",
    "\n",
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we concatenate the title and the text content of each article, creating the `full_text` column. We also create the `is_data_science` column, which indicates whether the article has the <font color=\"blue\">“Data Science”</font>tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two columns:\n",
    "# - full_text: contains the concatenation of the title and the text of the article.\n",
    "# - is_data_science: a boolean which is True if the article has the \"Data Science\" tag\n",
    "df_articles[\"is_data_science\"] = df_articles[\"tags\"] \\\n",
    "  .apply(lambda tags_list: \"Data Science\" in tags_list)\n",
    "df_articles[\"full_text\"] = df_articles[\"title\"] + \" \" + df_articles[\"text\"]\n",
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s then keep only `1,000` samples of articles with the “Data Science” tag and 1,000 samples without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 1000 articles is_data_science = True and 1000 articles with\n",
    "# is_data_science = False\n",
    "df = pd.concat([\n",
    "    df_articles[df_articles[\"is_data_science\"]].sample(n=1000),\n",
    "    df_articles[~df_articles[\"is_data_science\"]].sample(n=1000)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download a sentence embeddings model called `all-MiniLM-L6-v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the sentence embeddings model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "… and then use it to generate an embedding for each article in the dataset using the `full_text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed article texts\n",
    "corpus = df[\"full_text\"].values\n",
    "corpus_embeddings = embedder.encode(corpus)\n",
    "print(corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 2,000 embeddings (1,000 for articles with the “Data Science” tag, 1,000 for articles without it), each one with 384 dimensions (which is the number of dimensions of the embeddings produced with the specific `all-MiniLM-L6-v2` model).\n",
    "\n",
    "Let’s split the articles into training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "X = corpus_embeddings\n",
    "y = df[\"is_data_science\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, Let’s train a `LogisticRegression` model on the training set. And then, we can produce the predictions on the test set and use the `classification_report` utility function from `sklearn.metrics` to quickly see metrics like precision, recall, and F1 score.\n",
    "\n",
    "This part above requires you to build the code yourself. Just try your best! Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
