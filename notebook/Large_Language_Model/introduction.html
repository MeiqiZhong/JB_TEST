
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>8. Introduction &#8212; ZMQ&#39;s NLP</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebook/Large_Language_Model/introduction';</script>
    <link rel="canonical" href="https://meiqizhong.github.io/JB_TEST/notebook/Large_Language_Model/introduction.html" />
    <link rel="icon" href="../../_static/fav.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="9. Large Language Models Basic" href="basic/basic.html" />
    <link rel="prev" title="7. Project: Classify Medium Articles with Embeddings" href="../Natural_Language_Processing/assignments/embedding_project.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="ZMQ's NLP - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="ZMQ's NLP - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Start with NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Natural_Language_Processing/nlp.html">1. Natural Language Processing</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Natural_Language_Processing/text-preprocessing.html">2. Text Preprocessing</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Natural_Language_Processing/assignments/beginner-guide-to-text-preprocessing.html">2.6. Project: Beginner’s Guide to Text Pre-Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Natural_Language_Processing/assignments/getting-start-nlp-with-classification-task.html">2.7. Project: Getting Start NLP with classification task</a></li>

</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Word Embedding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Natural_Language_Processing/TF-IDF.html">3. TF-IDF（Term frequency-inverse document frequency ）</a></li>

<li class="toctree-l1"><a class="reference internal" href="../Natural_Language_Processing/word-embedding.html">5. Word embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Natural_Language_Processing/assignments/news-topic-classification-tasks.html">6. Project: News topic classification tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Natural_Language_Processing/assignments/embedding_project.html">7. Project: Classify Medium Articles with Embeddings</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Large Language Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">8. Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="basic/basic.html">9. Large Language Models Basic</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="basic/attention.html">9.1. Coding Attention Mechanisms</a></li>
<li class="toctree-l2"><a class="reference internal" href="basic/transformer.html">9.2. Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="basic/language-modelling.html">9.3. Transformers for Language Modelling</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pretrained-model/implementing-a-GPT-model.html">10. Implementing a GPT model from Scratch To Generate Text</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="pretrained-model/pretraining-on-unlabeled-data.html">10.10. Pretraining on Unlabeled Data</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/MeiqiZhong/JB_TEST/master?urlpath=tree/notebook/Large_Language_Model/introduction.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/MeiqiZhong/JB_TEST/blob/master/notebook/Large_Language_Model/introduction.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/MeiqiZhong/JB_TEST" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/MeiqiZhong/JB_TEST/edit/main/notebook/Large_Language_Model/introduction.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/MeiqiZhong/JB_TEST/issues/new?title=Issue%20on%20page%20%2Fnotebook/Large_Language_Model/introduction.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notebook/Large_Language_Model/introduction.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-large-language-models-llms">8.1. What are large language models (LLMs)?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#narrow-sense">8.1.1. Narrow sense</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broad-sense">8.1.2. Broad Sense</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-llms-work">8.2. How do LLMs Work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-of-llm">8.3. Architecture of LLM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constructing-llms-a-process-overview">8.4. Constructing LLMs: A Process Overview</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h1><span class="section-number">8. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h1>
<section id="what-are-large-language-models-llms">
<h2><span class="section-number">8.1. </span>What are large language models (LLMs)?<a class="headerlink" href="#what-are-large-language-models-llms" title="Link to this heading">#</a></h2>
<p>Large Language Models (LLMs) are advanced artificial intelligence systems that have been trained on vast amounts of text data to understand and generate human-like language. These models are typically based on deep learning architectures, such as transformer neural networks, and are capable of performing a wide range of natural language processing tasks.</p>
<p>Key characteristics of large language models include:</p>
<ol class="arabic simple">
<li><p><strong>Scale</strong>: LLMs are trained on massive datasets containing billions or even trillions of words. This extensive training corpus allows them to capture a broad understanding of language patterns and nuances.</p></li>
<li><p><strong>Complexity</strong>: These models are often deep neural networks with numerous layers and parameters, allowing them to learn intricate relationships within language data.</p></li>
<li><p><strong>Versatility</strong>: LLMs can be fine-tuned for various natural language processing tasks, including text generation, translation, summarization, sentiment analysis, question answering, and more.</p></li>
<li><p><strong>Generative Capabilities</strong>: One of the notable features of LLMs is their ability to generate coherent and contextually relevant text. Given a prompt or context, they can produce human-like responses or complete passages of text.</p></li>
<li><p><strong>Adaptability</strong>: LLMs can adapt to different domains or styles of language through fine-tuning or conditioning on specific data.</p></li>
<li><p><strong>Resource Intensiveness</strong>: Training and using large language models require significant computational resources, including powerful hardware and substantial amounts of data.</p></li>
<li><p><strong>Ethical and Societal Considerations</strong>: The development and deployment of LLMs raise ethical concerns related to biases in the training data, potential misuse for spreading misinformation, and the societal impacts of automated content generation.</p></li>
</ol>
<section id="narrow-sense">
<h3><span class="section-number">8.1.1. </span>Narrow sense<a class="headerlink" href="#narrow-sense" title="Link to this heading">#</a></h3>
<p>In the narrow sense, a large language model is described as a probabilistic model that assigns a probability to every finite sequence, whether it’s grammatical or not. This perspective emphasizes the probabilistic nature of language models, indicating that they can assign a likelihood to any sequence of tokens, regardless of whether it conforms to grammatical rules or not. This perspective highlights the fundamental nature of language models as probabilistic models that capture the statistical regularities of natural language.</p>
<figure class="align-default" id="id1">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/llm/implicit-order.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/llm/implicit-order.png" />
<figcaption>
<p><span class="caption-number">Fig. 8.1 </span><span class="caption-text">Implicit Order</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="broad-sense">
<h3><span class="section-number">8.1.2. </span>Broad Sense<a class="headerlink" href="#broad-sense" title="Link to this heading">#</a></h3>
<p>In the broad sense, large language models are categorized into different architectural types based on their structure and components:</p>
<ul class="simple">
<li><p><strong>Decoder-only models:</strong> These models, such as GPT (Generative Pre-trained Transformer), OPT (OpenAI’s Pre-trained Transformer), LLaMA, and PaLM, primarily consist of decoder layers. Decoder-only models are designed for tasks like text generation, where the model generates output tokens autoregressively based on preceding tokens. GPT-X is a notable example of a decoder-only model.</p></li>
<li><p><strong>Encoder-only models:</strong> Models like BERT (Bidirectional Encoder Representations from Transformers), RoBERTa, and ELECTRA are categorized as encoder-only models. These models focus on capturing contextual representations of input tokens without autoregressive generation. They are often used for tasks like text classification, where bidirectional context is essential.</p></li>
<li><p><strong>Encoder-decoder models:</strong> Architectures like T5 (Text-To-Text Transfer Transformer) and BART (Bidirectional and Auto-Regressive Transformers) include both encoder and decoder components. These models are versatile and can handle various tasks, including text generation, text summarization, translation, and more. They combine the strengths of both encoder and decoder architectures, enabling them to perform both generation and comprehension tasks.</p></li>
</ul>
<p>These models have demonstrated impressive capabilities in understanding and generating natural language, leading to their widespread adoption across various industries and applications.</p>
<figure class="align-default" id="id2">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/llm/clipboard_20240409_112330.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/llm/clipboard_20240409_112330.png" />
<figcaption>
<p><span class="caption-number">Fig. 8.2 </span><span class="caption-text">An Evolutionary Tree of Modern LLMs</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="how-do-llms-work">
<h2><span class="section-number">8.2. </span>How do LLMs Work?<a class="headerlink" href="#how-do-llms-work" title="Link to this heading">#</a></h2>
<p>Large Language Models (LLMs) function on the foundational principles of deep learning, harnessing neural network architectures to analyze and comprehend human languages.</p>
<p>Trained on extensive datasets using self-supervised learning techniques, LLMs excel at recognizing intricate patterns and relationships within diverse language data. These models are structured with multiple layers, incorporating feedforward layers, embedding layers, and attention layers. Utilizing attention mechanisms such as self-attention, LLMs assess the significance of individual tokens within a sequence. This process enables the model to grasp intricate dependencies and relationships among words, phrases, and sentences, thus facilitating its ability to process and understand natural language effectively.</p>
</section>
<section id="architecture-of-llm">
<h2><span class="section-number">8.3. </span>Architecture of LLM<a class="headerlink" href="#architecture-of-llm" title="Link to this heading">#</a></h2>
<p>A Large Language Model’s (LLM) architecture is a pivotal element shaped by various considerations, including the model’s intended objectives, available computational resources, and the nature of language processing tasks it is designed to tackle. The overall architecture of an LLM typically comprises multiple layers, encompassing feedforward layers, embedding layers, and attention layers. These layers work in tandem to process input text and generate predictions.</p>
<p>Several key components significantly influence the architecture of Large Language Models:</p>
<ol class="arabic simple">
<li><p><strong>Model Size and Parameter Count</strong>: The size of the model and the number of parameters it encompasses play a crucial role in determining its architectural design. Larger models with more parameters often have enhanced capacity to capture intricate language patterns and nuances.</p></li>
<li><p><strong>Input Representations</strong>: The representation of input text, such as tokenization and embedding methods, directly impacts the architecture of the LLM. Effective input representations facilitate the model’s ability to understand and process textual data accurately.</p></li>
<li><p><strong>Self-Attention Mechanisms</strong>: Many LLM architectures leverage self-attention mechanisms, such as the transformer architecture, to capture long-range dependencies within input sequences. Self-attention enables the model to weigh the importance of different tokens in a sequence, facilitating robust language understanding.</p></li>
<li><p><strong>Training Objectives</strong>: The specific objectives of LLM training, including pre-training and fine-tuning tasks, influence architectural choices. Different training objectives may require adjustments to the model’s architecture to optimize performance on targeted tasks.</p></li>
<li><p><strong>Computational Efficiency</strong>: Efficiency considerations, such as computational resources and inference speed, impact architectural decisions. Architectures that balance model complexity with computational efficiency are preferred, especially in practical applications where real-time processing is essential.</p></li>
<li><p><strong>Decoding and Output Generation</strong>: Architectural design also encompasses decoding mechanisms for generating output text. Techniques for output generation, such as beam search or nucleus sampling, influence the overall architecture and performance of the LLM.</p></li>
</ol>
<figure class="align-default" id="id3">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/llm/scaling-laws.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/llm/scaling-laws.png" />
<figcaption>
<p><span class="caption-number">Fig. 8.3 </span><span class="caption-text">Scaling Laws of LLMs</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="constructing-llms-a-process-overview">
<h2><span class="section-number">8.4. </span>Constructing LLMs: A Process Overview<a class="headerlink" href="#constructing-llms-a-process-overview" title="Link to this heading">#</a></h2>
<p>Building large-scale language models involves a multi-stage process, consisting of pre-training, supervised fine-tuning, reward shaping, and reinforcement learning.</p>
<ol class="arabic simple">
<li><p><strong>Pre-training</strong>: The initial phase involves training the language model on vast amounts of unlabeled text data using self-supervised learning techniques. During pre-training, the model learns to understand the structure and semantics of language by predicting missing words in sentences, predicting the next word in a sequence, or performing other language modeling tasks. This phase aims to equip the model with a broad understanding of language patterns and nuances.</p></li>
<li><p><strong>Supervised Fine-Tuning</strong>: Following pre-training, the model undergoes supervised fine-tuning on specific tasks or domains. Fine-tuning adjusts the model’s parameters to better fit the target task using labeled data. This phase involves training the model with annotated examples, allowing it to specialize in tasks such as text classification, sentiment analysis, or question answering. Supervised fine-tuning enhances the model’s performance on task-specific objectives and improves its ability to generalize to new data.</p></li>
<li><p><strong>Reward Shaping</strong>: In the reward shaping stage, the model is further refined through reinforcement learning techniques. Reward shaping involves defining a reward function that guides the model’s behavior towards desired outcomes. By providing feedback in the form of rewards or penalties, the model learns to optimize its actions to maximize cumulative rewards over time. Reward shaping helps improve the model’s decision-making capabilities and adaptability to dynamic environments.</p></li>
<li><p><strong>Reinforcement Learning</strong>: The final phase of model construction involves reinforcement learning, where the model interacts with its environment and learns through trial and error. Reinforcement learning algorithms enable the model to explore different actions and strategies, gradually improving its performance through experience. By receiving feedback based on the outcomes of its actions, the model iteratively adjusts its behavior to achieve optimal results. Reinforcement learning enhances the model’s ability to handle complex tasks and adapt to changing circumstances.</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebook/Large_Language_Model"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../Natural_Language_Processing/assignments/embedding_project.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Project: Classify Medium Articles with Embeddings</p>
      </div>
    </a>
    <a class="right-next"
       href="basic/basic.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Large Language Models Basic</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-large-language-models-llms">8.1. What are large language models (LLMs)?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#narrow-sense">8.1.1. Narrow sense</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broad-sense">8.1.2. Broad Sense</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-llms-work">8.2. How do LLMs Work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-of-llm">8.3. Architecture of LLM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constructing-llms-a-process-overview">8.4. Constructing LLMs: A Process Overview</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By ZMQ
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>